# HuggingFace åˆ° vLLM å®ç°è½¬æ¢å®Œæ•´æŒ‡å—

## ğŸ“‹ ç›®å½•
1. [æ ¸å¿ƒå¯¹åº”å…³ç³»](#æ ¸å¿ƒå¯¹åº”å…³ç³»)
2. [é€æ­¥è½¬æ¢æµç¨‹](#é€æ­¥è½¬æ¢æµç¨‹)
3. [è¯¦ç»†ä»£ç æ˜ å°„](#è¯¦ç»†ä»£ç æ˜ å°„)
4. [å¸¸è§è½¬æ¢æ¨¡å¼](#å¸¸è§è½¬æ¢æ¨¡å¼)
5. [å®Œæ•´è½¬æ¢æ¨¡æ¿](#å®Œæ•´è½¬æ¢æ¨¡æ¿)

---

## 1. æ ¸å¿ƒå¯¹åº”å…³ç³»

### 1.1 æ•´ä½“æ¶æ„æ˜ å°„

```
HuggingFace å®ç°                          vLLM å®ç°
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ class Qwen2ForRewardModel       â”‚    â”‚ class Qwen2ForRewardModel    â”‚
â”‚   (PreTrainedModel)             â”‚ â†’ â”‚   (nn.Module)                â”‚
â”‚                                 â”‚    â”‚                              â”‚
â”‚ - __init__(config)              â”‚    â”‚ - __init__(config,           â”‚
â”‚                                 â”‚    â”‚     cache_config,            â”‚
â”‚                                 â”‚    â”‚     quant_config,            â”‚
â”‚                                 â”‚    â”‚     lora_config)             â”‚
â”‚                                 â”‚    â”‚                              â”‚
â”‚ - self.model = Qwen2Model()     â”‚    â”‚ - self.model = Qwen2Model()  â”‚
â”‚                                 â”‚    â”‚                              â”‚
â”‚ - self.score = nn.Sequential(   â”‚    â”‚ - self.score = nn.Sequential(â”‚
â”‚     nn.Linear(),                â”‚    â”‚     ColumnParallelLinear(),  â”‚
â”‚     nn.ReLU(),                  â”‚    â”‚     ReLU(),  â† è‡ªå®šä¹‰         â”‚
â”‚     nn.Linear()                 â”‚    â”‚     RowParallelLinear()      â”‚
â”‚   )                             â”‚    â”‚   )                          â”‚
â”‚                                 â”‚    â”‚                              â”‚
â”‚ - forward(                      â”‚    â”‚ - forward(                   â”‚
â”‚     input_ids,                  â”‚    â”‚     input_ids,               â”‚
â”‚     attention_mask,             â”‚    â”‚     positions,               â”‚
â”‚     position_ids,               â”‚    â”‚     kv_caches,               â”‚
â”‚     past_key_values,            â”‚    â”‚     attn_metadata,           â”‚
â”‚     labels,                     â”‚    â”‚     intermediate_tensors     â”‚
â”‚     ...                         â”‚    â”‚   )                          â”‚
â”‚   )                             â”‚    â”‚                              â”‚
â”‚   â†’ è¿”å› SequenceClassifier     â”‚    â”‚   â†’ è¿”å› torch.Tensor        â”‚
â”‚      OutputWithPast             â”‚    â”‚                              â”‚
â”‚                                 â”‚    â”‚ - pooler(                    â”‚
â”‚ - å†…éƒ¨å¤„ç†pooling                â”‚    â”‚     hidden_states,           â”‚
â”‚   (åœ¨forwardä¸­)                  â”‚    â”‚     pooling_metadata         â”‚
â”‚                                 â”‚    â”‚   )                          â”‚
â”‚                                 â”‚    â”‚                              â”‚
â”‚ - è‡ªåŠ¨æƒé‡åŠ è½½                   â”‚    â”‚ - load_weights(weights)      â”‚
â”‚   (from_pretrained)             â”‚    â”‚   â† æ‰‹åŠ¨å®ç°                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 é€å±‚å¯¹åº”å…³ç³»

| ç»„ä»¶ | HuggingFace | vLLM | å…³é”®å·®å¼‚ |
|------|-------------|------|---------|
| **åŸºç±»** | `PreTrainedModel` | `nn.Module` | vLLMä¸éœ€è¦HFçš„æ¨¡å‹ç®¡ç†åŠŸèƒ½ |
| **åˆå§‹åŒ–å‚æ•°** | `config` | `config, cache_config, quant_config, lora_config` | vLLMéœ€è¦æ¨ç†ç›¸å…³é…ç½® |
| **çº¿æ€§å±‚** | `nn.Linear` | `ColumnParallelLinear` / `RowParallelLinear` | vLLMæ”¯æŒtensorå¹¶è¡Œ |
| **æ¿€æ´»å‡½æ•°** | `nn.ReLU()` | `ReLU()` (è‡ªå®šä¹‰) | vLLMéœ€è¦å¤„ç†å…ƒç»„è¾“å‡º |
| **å‰å‘å‚æ•°** | è®­ç»ƒå‹å¥½(labels, return_dict) | æ¨ç†ä¼˜åŒ–(kv_caches, attn_metadata) | ä¸åŒä½¿ç”¨åœºæ™¯ |
| **è¾“å‡ºæ ¼å¼** | ç»“æ„åŒ–å¯¹è±¡ | åŸå§‹å¼ é‡ | vLLMè¿½æ±‚æ€§èƒ½ |
| **æ± åŒ–** | å†…åµŒåœ¨forward | ç‹¬ç«‹çš„pooleræ–¹æ³• | vLLMè§£è€¦é€»è¾‘ |
| **æƒé‡åŠ è½½** | è‡ªåŠ¨åŒ– | æ‰‹åŠ¨å®ç° | vLLMéœ€è¦å¤„ç†å¹¶è¡ŒåŒ– |

---

## 2. é€æ­¥è½¬æ¢æµç¨‹

### æ­¥éª¤ 1: åˆ†æ HuggingFace å®ç°ç»“æ„

```python
# 1. è¯†åˆ«æ¨¡å‹çš„æ ¸å¿ƒç»„ä»¶
class Qwen2ForRewardModel(PreTrainedModel):
    def __init__(self, config):
        # æ‰¾å‡ºæ‰€æœ‰å­æ¨¡å—
        self.model = Qwen2Model(config)           # â† éª¨å¹²ç½‘ç»œ
        self.score = nn.Sequential(...)           # â† ä»»åŠ¡ç‰¹å®šå¤´
    
    def forward(self, ...):
        # æ‰¾å‡ºè®¡ç®—æµç¨‹
        hidden_states = self.model(...)           # â† ç‰¹å¾æå–
        logits = self.score(hidden_states)        # â† ä»»åŠ¡è®¡ç®—
        pooled_logits = logits[..., -1, :]       # â† æ± åŒ–é€»è¾‘
        return SequenceClassifierOutput(...)      # â† è¾“å‡ºæ ¼å¼
```

**åˆ†ææ¸…å•**:
- âœ… éª¨å¹²ç½‘ç»œ: `self.model`
- âœ… ä»»åŠ¡å¤´: `self.score`
- âœ… è®¡ç®—æµç¨‹: model â†’ score â†’ pooling
- âœ… è¾“å…¥å‚æ•°: input_ids, attention_mask, position_ids...
- âœ… è¾“å‡ºæ ¼å¼: SequenceClassifierOutputWithPast

### æ­¥éª¤ 2: åˆ›å»º vLLM åŸºç¡€æ¡†æ¶

```python
from vllm.model_executor.layers.linear import (
    ColumnParallelLinear, RowParallelLinear
)
from vllm.model_executor.layers.pooler import Pooler, PoolingType
from vllm.model_executor.models.qwen2 import Qwen2Model

class Qwen2ForRewardModel(nn.Module):
    # 1. æ·»åŠ vLLMç‰¹å®šçš„ç±»å±æ€§
    packed_modules_mapping = {...}      # â† å‚æ•°å †å æ˜ å°„
    supported_lora_modules = [...]      # â† LoRAæ”¯æŒ
    
    def __init__(
        self,
        config,
        cache_config=None,              # â† vLLMä¸“ç”¨
        quant_config=None,              # â† vLLMä¸“ç”¨
        lora_config=None,               # â† vLLMä¸“ç”¨
    ):
        super().__init__()
        # 2. åˆå§‹åŒ–é…ç½®
        self.config = config
        self.quant_config = quant_config
        
        # 3. åˆå§‹åŒ–éª¨å¹²ç½‘ç»œ
        self.model = Qwen2Model(config, cache_config, quant_config)
        
        # 4. åˆå§‹åŒ–ä»»åŠ¡å¤´ (å¾…è½¬æ¢)
        # 5. åˆå§‹åŒ–pooler (æ–°å¢)
```

### æ­¥éª¤ 3: è½¬æ¢çº¿æ€§å±‚ä¸ºå¹¶è¡Œå±‚

```python
# HuggingFace â†’ vLLM è½¬æ¢è§„åˆ™

# è§„åˆ™1: è¾“å‡ºå±‚ä½¿ç”¨ ColumnParallelLinear
nn.Linear(in_features, out_features, bias=True)
â†“
ColumnParallelLinear(
    in_features,
    out_features,
    bias=True,                          # ä¿æŒåŸæœ‰biasè®¾ç½®
    quant_config=quant_config,          # æ–°å¢: é‡åŒ–é…ç½®
)

# è§„åˆ™2: æœ€ç»ˆæŠ•å½±å±‚ä½¿ç”¨ RowParallelLinear
nn.Linear(in_features, out_features, bias=False)
â†“
RowParallelLinear(
    in_features,
    out_features,
    bias=False,
    quant_config=quant_config,
)

# è§„åˆ™3: ä¸­é—´çš„ReLUéœ€è¦è‡ªå®šä¹‰åŒ…è£…
nn.ReLU()
â†“
class ReLU(nn.Module):
    def __init__(self):
        super().__init__()
        self.activation = nn.ReLU()
    
    def forward(self, input):
        input, _ = input  # â† è§£åŒ…ColumnParallelLinearçš„å…ƒç»„è¾“å‡º
        return self.activation(input)
```

**å®é™…è½¬æ¢ä¾‹å­**:

```python
# ============ HuggingFace ç‰ˆæœ¬ ============
self.score = nn.Sequential(
    nn.Linear(config.hidden_size, config.hidden_size),
    nn.ReLU(),
    nn.Linear(config.hidden_size, 1)
)

# ============ vLLM ç‰ˆæœ¬ ============
self.score = nn.Sequential(
    ColumnParallelLinear(                    # ç¬¬1å±‚: åˆ—å¹¶è¡Œ
        config.hidden_size,
        config.hidden_size,
        quant_config=quant_config,
    ),
    ReLU(),                                   # æ¿€æ´»: è‡ªå®šä¹‰å¤„ç†å…ƒç»„
    RowParallelLinear(                        # ç¬¬2å±‚: è¡Œå¹¶è¡Œ
        config.hidden_size,
        1,
        quant_config=quant_config,
    ),
)
```

### æ­¥éª¤ 4: è½¬æ¢ forward æ–¹æ³•

```python
# ============ HuggingFace ç‰ˆæœ¬ ============
def forward(
    self,
    input_ids: torch.LongTensor = None,
    attention_mask: Optional[torch.Tensor] = None,
    position_ids: Optional[torch.LongTensor] = None,
    past_key_values: Optional[List[torch.FloatTensor]] = None,
    inputs_embeds: Optional[torch.FloatTensor] = None,
    labels: Optional[torch.LongTensor] = None,          # â† è®­ç»ƒç”¨
    use_cache: Optional[bool] = None,
    output_attentions: Optional[bool] = None,
    output_hidden_states: Optional[bool] = None,
    return_dict: Optional[bool] = None,
) -> Union[Tuple, SequenceClassifierOutputWithPast]:
    # 1. è°ƒç”¨éª¨å¹²ç½‘ç»œ
    transformer_outputs = self.model(
        input_ids,
        attention_mask=attention_mask,
        position_ids=position_ids,
        past_key_values=past_key_values,
        ...
    )
    hidden_states = transformer_outputs[0]
    
    # 2. è®¡ç®—logits
    logits = self.score(hidden_states)
    
    # 3. æ± åŒ– (æå–æœ€åä¸€ä¸ªtoken)
    sequence_lengths = ... # è®¡ç®—æœ‰æ•ˆé•¿åº¦
    pooled_logits = logits[torch.arange(batch_size), sequence_lengths]
    
    # 4. è®¡ç®—æŸå¤± (å¦‚æœæœ‰labels)
    loss = None
    if labels is not None:
        loss_fct = MSELoss()
        loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())
    
    # 5. è¿”å›ç»“æ„åŒ–è¾“å‡º
    return SequenceClassifierOutputWithPast(
        loss=loss,
        logits=pooled_logits,
        past_key_values=transformer_outputs.past_key_values,
        ...
    )

# ============ vLLM ç‰ˆæœ¬ ============
def forward(
    self,
    input_ids: torch.Tensor,
    positions: torch.Tensor,                             # â† ç®€åŒ–çš„ä½ç½®ä¿¡æ¯
    kv_caches: List[torch.Tensor],                      # â† vLLMçš„KVç¼“å­˜
    attn_metadata: AttentionMetadata,                   # â† æ³¨æ„åŠ›å…ƒæ•°æ®
    intermediate_tensors: Optional[IntermediateTensors] = None,  # â† æµæ°´çº¿å¹¶è¡Œ
) -> torch.Tensor:                                      # â† ç®€å•è¿”å›å¼ é‡
    # 1. è°ƒç”¨éª¨å¹²ç½‘ç»œ
    hidden_states = self.model(
        input_ids,
        positions,
        kv_caches,
        attn_metadata,
        intermediate_tensors
    )
    
    # 2. è®¡ç®—logits
    logits, _ = self.score(hidden_states)  # â† æ³¨æ„è§£åŒ…å…ƒç»„
    
    # 3. ç›´æ¥è¿”å› (æ± åŒ–åœ¨å•ç‹¬çš„pooleræ–¹æ³•ä¸­)
    return logits

# æ–°å¢: ç‹¬ç«‹çš„pooleræ–¹æ³•
def pooler(
    self,
    hidden_states: torch.Tensor,
    pooling_metadata: PoolingMetadata,
) -> Optional[PoolerOutput]:
    return self._pooler(hidden_states, pooling_metadata)
```

**è½¬æ¢è¦ç‚¹**:
- âŒ ç§»é™¤è®­ç»ƒç›¸å…³å‚æ•°: `labels`, `return_dict`
- âŒ ç§»é™¤è¾“å‡ºæ§åˆ¶å‚æ•°: `output_attentions`, `output_hidden_states`
- âœ… ä½¿ç”¨vLLMå‚æ•°: `positions`, `kv_caches`, `attn_metadata`
- âœ… ç®€åŒ–è¿”å›å€¼: ç›´æ¥è¿”å›å¼ é‡ï¼Œä¸è¿”å›æŸå¤±å’Œå…ƒæ•°æ®
- âœ… æ± åŒ–è§£è€¦: ä»forwardç§»åˆ°ç‹¬ç«‹çš„pooleræ–¹æ³•

### æ­¥éª¤ 5: å®ç°æƒé‡åŠ è½½

```python
def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
    """
    æ ¸å¿ƒé€»è¾‘:
    1. å®šä¹‰å‚æ•°æ˜ å°„å…³ç³»
    2. éå†æ‰€æœ‰æƒé‡
    3. å¤„ç†å‚æ•°åæ˜ å°„
    4. è°ƒç”¨æƒé‡åŠ è½½å™¨
    """
    
    # 1. å®šä¹‰å †å å‚æ•°æ˜ å°„
    stacked_params_mapping = [
        ("qkv_proj", "q_proj", "q"),
        ("qkv_proj", "k_proj", "k"),
        ("qkv_proj", "v_proj", "v"),
        ("gate_up_proj", "gate_proj", 0),
        ("gate_up_proj", "up_proj", 1),
    ]
    
    # 2. è·å–æ¨¡å‹å‚æ•°å­—å…¸
    params_dict = dict(self.named_parameters(remove_duplicate=False))
    
    # 3. éå†æƒé‡
    for name, loaded_weight in weights:
        # 3.1 è·³è¿‡ä¸éœ€è¦çš„æƒé‡
        if name == "lm_head.weight":
            continue
        if "rotary_emb.inv_freq" in name:
            continue
        
        # 3.2 å¤„ç†å †å å‚æ•°
        for (param_name, weight_name, shard_id) in stacked_params_mapping:
            if weight_name not in name:
                continue
            
            # æ›¿æ¢å‚æ•°å
            name = name.replace(weight_name, param_name)
            
            # è·³è¿‡ä¸å­˜åœ¨çš„bias
            if name.endswith(".bias") and name not in params_dict:
                continue
            
            # åŠ è½½æƒé‡
            param = params_dict[name]
            weight_loader = param.weight_loader
            weight_loader(param, loaded_weight, shard_id)
            break
        else:
            # 3.3 å¤„ç†å¸¸è§„å‚æ•°
            if name.endswith(".bias") and name not in params_dict:
                continue
            
            name = maybe_remap_kv_scale_name(name, params_dict)
            if name is None:
                continue
            
            param = params_dict[name]
            weight_loader = getattr(param, "weight_loader", default_weight_loader)
            weight_loader(param, loaded_weight)
```

### æ­¥éª¤ 6: æ·»åŠ å¿…è¦çš„ç±»å±æ€§

```python
class Qwen2ForRewardModel(nn.Module):
    # 1. å‚æ•°å †å æ˜ å°„ (ç”¨äºæƒé‡åŠ è½½)
    packed_modules_mapping = {
        "qkv_proj": ["q_proj", "k_proj", "v_proj"],
        "gate_up_proj": ["gate_proj", "up_proj"],
    }
    
    # 2. LoRAæ”¯æŒçš„æ¨¡å—
    supported_lora_modules = [
        "qkv_proj",
        "o_proj",
        "gate_up_proj",
        "down_proj",
    ]
    
    # 3. Embeddingç›¸å…³ (é€šå¸¸ä¸ºç©º)
    embedding_modules = {}
    embedding_padding_modules = []
```

---

## 3. è¯¦ç»†ä»£ç æ˜ å°„è¡¨

### 3.1 åˆå§‹åŒ–æ–¹æ³•æ˜ å°„

| æ­¥éª¤ | HuggingFace | vLLM | è¯´æ˜ |
|------|-------------|------|------|
| 1 | `super().__init__(config)` | `super().__init__()` | vLLMä¸ç»§æ‰¿PreTrainedModel |
| 2 | `self.config = config` | `self.config = config`<br>`self.quant_config = quant_config`<br>`self.lora_config = lora_config` | vLLMéœ€è¦é¢å¤–é…ç½® |
| 3 | `self.model = Qwen2Model(config)` | `self.model = Qwen2Model(config, cache_config, quant_config)` | vLLMä¼ å…¥æ¨ç†é…ç½® |
| 4 | `self.score = nn.Sequential(`<br>`  nn.Linear(...),`<br>`  nn.ReLU(),`<br>`  nn.Linear(...)`<br>`)` | `self.score = nn.Sequential(`<br>`  ColumnParallelLinear(...),`<br>`  ReLU(),`<br>`  RowParallelLinear(...)`<br>`)` | ä½¿ç”¨å¹¶è¡Œå±‚ |
| 5 | æ—  | `self._pooler = Pooler(...)` | vLLMæ–°å¢pooler |
| 6 | `self.post_init()` | æ—  | vLLMä¸éœ€è¦ |

### 3.2 Forwardæ–¹æ³•æ˜ å°„

| éƒ¨åˆ† | HuggingFace | vLLM | è½¬æ¢è¯´æ˜ |
|------|-------------|------|---------|
| **å‚æ•°** | `input_ids, attention_mask, position_ids, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict` | `input_ids, positions, kv_caches, attn_metadata, intermediate_tensors` | ç§»é™¤è®­ç»ƒ/è°ƒè¯•å‚æ•°ï¼Œä½¿ç”¨vLLMæ¨ç†å‚æ•° |
| **éª¨å¹²è°ƒç”¨** | `transformer_outputs = self.model(...)`<br>`hidden_states = transformer_outputs[0]` | `hidden_states = self.model(...)` | vLLMç›´æ¥è¿”å›å¼ é‡ |
| **Scoreè®¡ç®—** | `logits = self.score(hidden_states)` | `logits, _ = self.score(hidden_states)` | vLLMéœ€è¦è§£åŒ…å…ƒç»„ |
| **æ± åŒ–** | åœ¨forwardå†…éƒ¨:<br>`sequence_lengths = ...`<br>`pooled = logits[..., sequence_lengths]` | ç§»åˆ°pooleræ–¹æ³•:<br>`def pooler(self, ...)` | é€»è¾‘è§£è€¦ |
| **æŸå¤±è®¡ç®—** | `if labels is not None:`<br>`  loss = loss_fct(...)` | æ—  | vLLMçº¯æ¨ç†ï¼Œä¸è®¡ç®—æŸå¤± |
| **è¿”å›å€¼** | `return SequenceClassifierOutputWithPast(...)` | `return logits` | vLLMè¿”å›ç®€å•å¼ é‡ |

### 3.3 çº¿æ€§å±‚æ˜ å°„è§„åˆ™

```python
# æ˜ å°„è§„åˆ™è¡¨
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ä½ç½®               HuggingFace              vLLM                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ç¬¬ä¸€å±‚/ä¸­é—´å±‚      nn.Linear(in, out)       ColumnParallelLinear    â”‚
â”‚                                             - è·¨GPUæŒ‰åˆ—åˆ‡åˆ†          â”‚
â”‚                                             - è¾“å‡º(tensor, bias)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æœ€åä¸€å±‚           nn.Linear(in, out)       RowParallelLinear       â”‚
â”‚                                             - è·¨GPUæŒ‰è¡Œåˆ‡åˆ†          â”‚
â”‚                                             - éœ€è¦AllReduce         â”‚
â”‚                                             - è¾“å‡º(tensor, bias)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¿€æ´»å‡½æ•°           nn.ReLU()                è‡ªå®šä¹‰ReLU()            â”‚
â”‚                                             - è§£åŒ…å…ƒç»„è¾“å…¥          â”‚
â”‚                                             - åªå¤„ç†tensoréƒ¨åˆ†       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. å¸¸è§è½¬æ¢æ¨¡å¼

### æ¨¡å¼ 1: åºåˆ—åˆ†ç±»æ¨¡å‹

```python
# ========== HuggingFace æ¨¡å¼ ==========
class ModelForSequenceClassification(PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.model = BaseModel(config)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)
    
    def forward(self, input_ids, attention_mask, ...):
        outputs = self.model(input_ids, attention_mask, ...)
        hidden = outputs[0][:, -1, :]  # å–æœ€åä¸€ä¸ªtoken
        logits = self.classifier(hidden)
        return SequenceClassifierOutput(logits=logits, ...)

# ========== vLLM æ¨¡å¼ ==========
class ModelForSequenceClassification(nn.Module):
    def __init__(self, config, cache_config, quant_config, lora_config):
        super().__init__()
        self.model = BaseModel(config, cache_config, quant_config)
        self.classifier = RowParallelLinear(  # â† ä½¿ç”¨è¡Œå¹¶è¡Œ
            config.hidden_size,
            config.num_labels,
            quant_config=quant_config,
        )
        self._pooler = Pooler(pooling_type=PoolingType.LAST, normalize=False)
    
    def forward(self, input_ids, positions, kv_caches, attn_metadata, ...):
        hidden_states = self.model(input_ids, positions, kv_caches, attn_metadata)
        logits, _ = self.classifier(hidden_states)  # â† è§£åŒ…å…ƒç»„
        return logits
    
    def pooler(self, hidden_states, pooling_metadata):
        return self._pooler(hidden_states, pooling_metadata)
```

### æ¨¡å¼ 2: å¤šå±‚MLPå¤´

```python
# ========== HuggingFace æ¨¡å¼ ==========
self.head = nn.Sequential(
    nn.Linear(hidden, intermediate),
    nn.GELU(),
    nn.Dropout(0.1),
    nn.Linear(intermediate, output),
)

# ========== vLLM æ¨¡å¼ ==========
# æ–¹æ³•1: å¦‚æœä¸éœ€è¦dropout (æ¨ç†æ—¶)
self.head = nn.Sequential(
    ColumnParallelLinear(hidden, intermediate, quant_config=quant_config),
    nn.GELU(),  # æ ‡å‡†æ¿€æ´»å¯ç›´æ¥ç”¨
    RowParallelLinear(intermediate, output, quant_config=quant_config),
)

# æ–¹æ³•2: å¦‚æœéœ€è¦å¤„ç†dropout
class CustomGELU(nn.Module):
    def forward(self, input):
        input, _ = input
        return nn.functional.gelu(input)

self.head = nn.Sequential(
    ColumnParallelLinear(hidden, intermediate, quant_config=quant_config),
    CustomGELU(),
    RowParallelLinear(intermediate, output, quant_config=quant_config),
)
```

### æ¨¡å¼ 3: å¥–åŠ±æ¨¡å‹ (å®Œæ•´ç¤ºä¾‹)

```python
# ========== HuggingFace ç‰ˆæœ¬ ==========
class RewardModel(PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.model = Transformer(config)
        self.score = nn.Sequential(
            nn.Linear(config.hidden_size, config.hidden_size),
            nn.ReLU(),
            nn.Linear(config.hidden_size, 1)
        )
        self.post_init()
    
    def forward(self, input_ids, attention_mask, labels=None, ...):
        outputs = self.model(input_ids, attention_mask=attention_mask, ...)
        hidden = outputs[0]
        logits = self.score(hidden)
        
        # æ‰¾åˆ°æœ€åä¸€ä¸ªépadding token
        if self.config.pad_token_id is None:
            sequence_lengths = -1
        else:
            sequence_lengths = (
                torch.eq(input_ids, self.config.pad_token_id)
                .int().argmax(-1) - 1
            )
        
        pooled_logits = logits[torch.arange(batch_size), sequence_lengths]
        
        loss = None
        if labels is not None:
            loss = nn.MSELoss()(pooled_logits, labels)
        
        return SequenceClassifierOutputWithPast(
            loss=loss,
            logits=pooled_logits,
            past_key_values=outputs.past_key_values,
        )

# ========== vLLM ç‰ˆæœ¬ ==========
class RewardModel(nn.Module):
    packed_modules_mapping = {
        "qkv_proj": ["q_proj", "k_proj", "v_proj"],
        "gate_up_proj": ["gate_proj", "up_proj"],
    }
    
    supported_lora_modules = ["qkv_proj", "o_proj", "gate_up_proj", "down_proj"]
    
    def __init__(self, config, cache_config, quant_config, lora_config):
        super().__init__()
        self.config = config
        self.quant_config = quant_config
        
        self.model = Transformer(config, cache_config, quant_config)
        
        # è‡ªå®šä¹‰ReLUå¤„ç†å…ƒç»„
        class ReLU(nn.Module):
            def __init__(self):
                super().__init__()
                self.activation = nn.ReLU()
            def forward(self, input):
                input, _ = input
                return self.activation(input)
        
        self.score = nn.Sequential(
            ColumnParallelLinear(
                config.hidden_size,
                config.hidden_size,
                quant_config=quant_config,
            ),
            ReLU(),
            RowParallelLinear(
                config.hidden_size,
                1,
                quant_config=quant_config,
            ),
        )
        
        self._pooler = Pooler(pooling_type=PoolingType.ALL, normalize=False)
    
    def forward(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors=None):
        hidden_states = self.model(
            input_ids, positions, kv_caches, attn_metadata, intermediate_tensors
        )
        logits, _ = self.score(hidden_states)
        return logits
    
    def pooler(self, hidden_states, pooling_metadata):
        return self._pooler(hidden_states, pooling_metadata)
    
    def load_weights(self, weights):
        stacked_params_mapping = [
            ("qkv_proj", "q_proj", "q"),
            ("qkv_proj", "k_proj", "k"),
            ("qkv_proj", "v_proj", "v"),
            ("gate_up_proj", "gate_proj", 0),
            ("gate_up_proj", "up_proj", 1),
        ]
        
        params_dict = dict(self.named_parameters(remove_duplicate=False))
        
        for name, loaded_weight in weights:
            if name == "lm_head.weight" or "rotary_emb.inv_freq" in name:
                continue
            
            for (param_name, weight_name, shard_id) in stacked_params_mapping:
                if weight_name not in name:
                    continue
                name = name.replace(weight_name, param_name)
                if name.endswith(".bias") and name not in params_dict:
                    continue
                param = params_dict[name]
                param.weight_loader(param, loaded_weight, shard_id)
                break
            else:
                if name.endswith(".bias") and name not in params_dict:
                    continue
                name = maybe_remap_kv_scale_name(name, params_dict)
                if name is None:
                    continue
                param = params_dict[name]
                weight_loader = getattr(param, "weight_loader", default_weight_loader)
                weight_loader(param, loaded_weight)
```

---

## 5. å®Œæ•´è½¬æ¢æ¨¡æ¿

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# vLLM æ¨¡å‹å®ç°æ¨¡æ¿
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from typing import Iterable, List, Optional, Tuple
import torch
from torch import nn

# vLLM imports
from vllm.attention import AttentionMetadata
from vllm.config import CacheConfig, LoRAConfig
from vllm.model_executor.layers.linear import ColumnParallelLinear, RowParallelLinear
from vllm.model_executor.layers.pooler import Pooler, PoolingType
from vllm.model_executor.layers.quantization.base_config import QuantizationConfig
from vllm.model_executor.model_loader.weight_utils import (
    default_weight_loader, maybe_remap_kv_scale_name
)
from vllm.model_executor.models.xxx import BaseModel  # æ›¿æ¢ä¸ºå®é™…éª¨å¹²ç½‘ç»œ
from vllm.model_executor.pooling_metadata import PoolingMetadata
from vllm.sequence import IntermediateTensors, PoolerOutput
from .utils import is_pp_missing_parameter


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ­¥éª¤1: è‡ªå®šä¹‰æ¿€æ´»å‡½æ•° (å¦‚æœä½¿ç”¨å¹¶è¡Œå±‚çš„è¯)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class CustomActivation(nn.Module):
    """
    åŒ…è£…æ ‡å‡†æ¿€æ´»å‡½æ•°ä»¥å¤„ç†å¹¶è¡Œå±‚çš„å…ƒç»„è¾“å‡º
    æ ¹æ®éœ€è¦æ›¿æ¢ä¸º ReLU, GELU, SiLU ç­‰
    """
    def __init__(self):
        super().__init__()
        self.activation = nn.ReLU()  # æˆ– nn.GELU(), nn.SiLU() ç­‰
    
    def forward(self, input):
        # è§£åŒ…å¹¶è¡Œå±‚çš„ (tensor, bias) å…ƒç»„
        if isinstance(input, tuple):
            input, _ = input
        return self.activation(input)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ­¥éª¤2: ä¸»æ¨¡å‹ç±»
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class YourModelForTask(nn.Module):
    """
    [æ¨¡å‹åç§°] - vLLMä¼˜åŒ–å®ç°
    
    ç”¨é€”: [æè¿°æ¨¡å‹çš„ä»»åŠ¡ï¼Œå¦‚åˆ†ç±»/å›å½’/å¥–åŠ±æ¨¡å‹ç­‰]
    
    åŸå§‹å®ç°: [HuggingFaceæ¨¡å‹é“¾æ¥]
    """
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ç±»å±æ€§é…ç½®
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # 1. å‚æ•°å †å æ˜ å°„ (ç”¨äºæƒé‡åŠ è½½)
    # æ ¹æ®éª¨å¹²ç½‘ç»œç±»å‹è°ƒæ•´ï¼Œå¸¸è§çš„æœ‰:
    packed_modules_mapping = {
        # Attentionå±‚çš„QKVåˆå¹¶
        "qkv_proj": ["q_proj", "k_proj", "v_proj"],
        # FFNå±‚çš„gateå’Œupåˆå¹¶ (ç”¨äºSwiGLU)
        "gate_up_proj": ["gate_proj", "up_proj"],
    }
    
    # 2. LoRAæ”¯æŒçš„æ¨¡å—åˆ—è¡¨
    supported_lora_modules = [
        "qkv_proj",
        "o_proj",
        "gate_up_proj",
        "down_proj",
        # å¦‚æœä»»åŠ¡å¤´ä¹Ÿæ”¯æŒLoRAï¼Œæ·»åŠ ç›¸åº”æ¨¡å—
        # "classifier",
    ]
    
    # 3. Embeddingç›¸å…³æ¨¡å— (é€šå¸¸ä¸ºç©ºï¼Œé™¤éæœ‰ç‰¹æ®Šembedding)
    embedding_modules = {}
    embedding_padding_modules = []
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # åˆå§‹åŒ–æ–¹æ³•
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def __init__(
        self,
        config,                                  # æ¨¡å‹é…ç½®å¯¹è±¡
        cache_config: Optional[CacheConfig] = None,      # KVç¼“å­˜é…ç½®
        quant_config: Optional[QuantizationConfig] = None,  # é‡åŒ–é…ç½®
        lora_config: Optional[LoRAConfig] = None,        # LoRAé…ç½®
    ) -> None:
        super().__init__()
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 1. ä¿å­˜é…ç½®
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.config = config
        self.lora_config = lora_config
        self.quant_config = quant_config
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 2. åˆå§‹åŒ–éª¨å¹²ç½‘ç»œ
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # æ›¿æ¢ä¸ºå®é™…çš„éª¨å¹²ç½‘ç»œç±»ï¼Œå¦‚:
        # - Qwen2Model, LlamaModel, MistralModel ç­‰
        self.model = BaseModel(config, cache_config, quant_config)
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 3. åˆå§‹åŒ–ä»»åŠ¡ç‰¹å®šçš„å¤´éƒ¨
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        
        # ç¤ºä¾‹A: å•å±‚åˆ†ç±»å¤´
        # self.classifier = RowParallelLinear(
        #     config.hidden_size,
        #     config.num_labels,
        #     quant_config=quant_config,
        # )
        
        # ç¤ºä¾‹B: å¤šå±‚MLPå¤´
        # self.head = nn.Sequential(
        #     ColumnParallelLinear(
        #         config.hidden_size,
        #         config.intermediate_size,
        #         quant_config=quant_config,
        #     ),
        #     CustomActivation(),
        #     RowParallelLinear(
        #         config.intermediate_size,
        #         config.num_labels,
        #         quant_config=quant_config,
        #     ),
        # )
        
        # ç¤ºä¾‹C: å¥–åŠ±æ¨¡å‹å¤´ (ä¸¤å±‚MLP)
        self.score = nn.Sequential(
            ColumnParallelLinear(
                config.hidden_size,
                config.hidden_size,
                quant_config=quant_config,
            ),
            CustomActivation(),
            RowParallelLinear(
                config.hidden_size,
                1,  # è¾“å‡ºç»´åº¦ï¼Œæ ¹æ®ä»»åŠ¡è°ƒæ•´
                quant_config=quant_config,
            ),
        )
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 4. åˆå§‹åŒ–Pooler
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # æ ¹æ®ä»»åŠ¡é€‰æ‹©æ± åŒ–ç±»å‹:
        # - PoolingType.LAST: å–æœ€åä¸€ä¸ªtoken (åºåˆ—åˆ†ç±»)
        # - PoolingType.ALL: ä¿ç•™æ‰€æœ‰token (tokenåˆ†ç±»/ç”Ÿæˆ)
        # - PoolingType.CLS: å–CLS token
        self._pooler = Pooler(
            pooling_type=PoolingType.LAST,  # æ ¹æ®ä»»åŠ¡è°ƒæ•´
            normalize=False,  # æ˜¯å¦L2å½’ä¸€åŒ–
        )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # å‰å‘ä¼ æ’­æ–¹æ³•
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def forward(
        self,
        input_ids: torch.Tensor,                            # [batch, seq_len]
        positions: torch.Tensor,                            # [batch, seq_len]
        kv_caches: List[torch.Tensor],                     # List of KV caches
        attn_metadata: AttentionMetadata,                  # æ³¨æ„åŠ›å…ƒæ•°æ®
        intermediate_tensors: Optional[IntermediateTensors] = None,  # æµæ°´çº¿å¹¶è¡Œ
    ) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            input_ids: è¾“å…¥token ID
            positions: tokenä½ç½®ç´¢å¼•
            kv_caches: æ¯å±‚çš„KVç¼“å­˜
            attn_metadata: æ³¨æ„åŠ›è®¡ç®—å…ƒæ•°æ®
            intermediate_tensors: æµæ°´çº¿å¹¶è¡Œçš„ä¸­é—´å¼ é‡
        
        Returns:
            torch.Tensor: æ¨¡å‹è¾“å‡º (å…·ä½“å½¢çŠ¶æ ¹æ®ä»»åŠ¡è€Œå®š)
        """
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 1. é€šè¿‡éª¨å¹²ç½‘ç»œæå–ç‰¹å¾
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        hidden_states = self.model(
            input_ids,
            positions,
            kv_caches,
            attn_metadata,
            intermediate_tensors
        )
        # hidden_states: [batch, seq_len, hidden_size]
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 2. é€šè¿‡ä»»åŠ¡å¤´è®¡ç®—è¾“å‡º
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # æ³¨æ„: å¹¶è¡Œå±‚è¿”å› (tensor, bias) å…ƒç»„ï¼Œéœ€è¦è§£åŒ…
        logits, _ = self.score(hidden_states)
        # logits: [batch, seq_len, output_dim]
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 3. è¿”å›logits (æ± åŒ–åœ¨pooleræ–¹æ³•ä¸­å•ç‹¬å¤„ç†)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        return logits
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Pooleræ–¹æ³• (æå–æœ€ç»ˆè¾“å‡º)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def pooler(
        self,
        hidden_states: torch.Tensor,                       # æ¨¡å‹è¾“å‡º
        pooling_metadata: PoolingMetadata,                 # æ± åŒ–å…ƒæ•°æ®
    ) -> Optional[PoolerOutput]:
        """
        æ± åŒ–æ“ä½œï¼šä»åºåˆ—è¾“å‡ºä¸­æå–æœ€ç»ˆç»“æœ
        
        Args:
            hidden_states: forwardæ–¹æ³•çš„è¾“å‡º
            pooling_metadata: åŒ…å«åºåˆ—é•¿åº¦ã€è¾¹ç•Œç­‰ä¿¡æ¯
        
        Returns:
            PoolerOutput: æ± åŒ–åçš„è¾“å‡º
        """
        return self._pooler(hidden_states, pooling_metadata)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # æƒé‡åŠ è½½æ–¹æ³•
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
        """
        ä»HuggingFace checkpointåŠ è½½æƒé‡
        
        å¤„ç†:
        1. å‚æ•°åæ˜ å°„ (HF â†’ vLLM)
        2. å †å å‚æ•°çš„æ­£ç¡®åŠ è½½
        3. å¹¶è¡Œåˆ‡åˆ†
        4. é‡åŒ–å‚æ•°å¤„ç†
        
        Args:
            weights: è¿­ä»£å™¨ï¼Œäº§ç”Ÿ (å‚æ•°å, æƒé‡å¼ é‡) å¯¹
        """
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 1. å®šä¹‰å †å å‚æ•°æ˜ å°„
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # æ ¼å¼: (vLLMå‚æ•°å, HFå‚æ•°å, shardæ ‡è¯†)
        stacked_params_mapping = [
            # QKVå †å 
            ("qkv_proj", "q_proj", "q"),
            ("qkv_proj", "k_proj", "k"),
            ("qkv_proj", "v_proj", "v"),
            # Gate-Upå †å 
            ("gate_up_proj", "gate_proj", 0),
            ("gate_up_proj", "up_proj", 1),
        ]
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 2. è·å–æ¨¡å‹å‚æ•°å­—å…¸
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        params_dict = dict(self.named_parameters(remove_duplicate=False))
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 3. éå†å¹¶åŠ è½½æƒé‡
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for name, loaded_weight in weights:
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # 3.1 è·³è¿‡ä¸éœ€è¦çš„æƒé‡
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            
            # è·³è¿‡è¯­è¨€æ¨¡å‹å¤´ (å¦‚æœä»»åŠ¡ä¸éœ€è¦)
            if name == "lm_head.weight":
                continue
            
            # è·³è¿‡æ—‹è½¬ä½ç½®ç¼–ç çš„è®¡ç®—å‚æ•°
            if "rotary_emb.inv_freq" in name:
                continue
            
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # 3.2 å¤„ç†å †å å‚æ•°
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            for (param_name, weight_name, shard_id) in stacked_params_mapping:
                if weight_name not in name:
                    continue
                
                # æ›¿æ¢å‚æ•°å
                name = name.replace(weight_name, param_name)
                
                # è·³è¿‡ä¸å­˜åœ¨çš„bias (GPTQé‡åŒ–æ¨¡å‹)
                if name.endswith(".bias") and name not in params_dict:
                    continue
                
                # æ£€æŸ¥æµæ°´çº¿å¹¶è¡Œ (æ˜¯å¦å±äºå½“å‰stage)
                if is_pp_missing_parameter(name, self):
                    continue
                
                # è·å–å‚æ•°å¹¶åŠ è½½
                param = params_dict[name]
                weight_loader = param.weight_loader
                weight_loader(param, loaded_weight, shard_id)
                break
            else:
                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # 3.3 å¤„ç†å¸¸è§„å‚æ•°
                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                
                # è·³è¿‡GPTQ bias
                if name.endswith(".bias") and name not in params_dict:
                    continue
                
                # é‡æ˜ å°„FP8é‡åŒ–çš„kv_scaleå‚æ•°
                name = maybe_remap_kv_scale_name(name, params_dict)
                if name is None:
                    continue
                
                # æ£€æŸ¥æµæ°´çº¿å¹¶è¡Œ
                if is_pp_missing_parameter(name, self):
                    continue
                
                # åŠ è½½æƒé‡
                param = params_dict[name]
                weight_loader = getattr(
                    param,
                    "weight_loader",
                    default_weight_loader
                )
                weight_loader(param, loaded_weight)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è½¬æ¢æ£€æŸ¥æ¸…å•
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
è½¬æ¢å®Œæˆåï¼Œè¯·æ£€æŸ¥ä»¥ä¸‹é¡¹ç›®:

âœ… ç±»å®šä¹‰
   â–¡ ç»§æ‰¿è‡ª nn.Module (ä¸æ˜¯ PreTrainedModel)
   â–¡ æ·»åŠ äº† packed_modules_mapping
   â–¡ æ·»åŠ äº† supported_lora_modules
   â–¡ æ·»åŠ äº† embedding_modules (é€šå¸¸ä¸ºç©º)

âœ… __init__ æ–¹æ³•
   â–¡ å‚æ•°åŒ…å«: config, cache_config, quant_config, lora_config
   â–¡ ä¿å­˜äº†æ‰€æœ‰é…ç½®åˆ° self
   â–¡ éª¨å¹²ç½‘ç»œä¼ å…¥äº† cache_config å’Œ quant_config
   â–¡ ä½¿ç”¨ ColumnParallelLinear / RowParallelLinear
   â–¡ è‡ªå®šä¹‰æ¿€æ´»å‡½æ•°å¤„ç†å…ƒç»„è¾“å…¥
   â–¡ åˆå§‹åŒ–äº† self._pooler

âœ… forward æ–¹æ³•
   â–¡ å‚æ•°: input_ids, positions, kv_caches, attn_metadata, intermediate_tensors
   â–¡ ç§»é™¤äº†è®­ç»ƒç›¸å…³å‚æ•° (labels, return_dict ç­‰)
   â–¡ éª¨å¹²ç½‘ç»œè°ƒç”¨ä¼ å…¥äº†æ‰€æœ‰vLLMå‚æ•°
   â–¡ è§£åŒ…å¹¶è¡Œå±‚çš„å…ƒç»„è¾“å‡º: logits, _ = self.score(...)
   â–¡ è¿”å›ç®€å•å¼ é‡ (ä¸æ˜¯å­—å…¸æˆ–å¯¹è±¡)

âœ… pooler æ–¹æ³•
   â–¡ å®ç°äº† pooler æ–¹æ³•
   â–¡ æ¥å— hidden_states å’Œ pooling_metadata
   â–¡ è°ƒç”¨ self._pooler å¹¶è¿”å›ç»“æœ

âœ… load_weights æ–¹æ³•
   â–¡ å®šä¹‰äº† stacked_params_mapping
   â–¡ è·å–äº† params_dict
   â–¡ è·³è¿‡ lm_head.weight (å¦‚æœä¸éœ€è¦)
   â–¡ è·³è¿‡ rotary_emb.inv_freq
   â–¡ å¤„ç†å †å å‚æ•°æ˜ å°„
   â–¡ å¤„ç†å¸¸è§„å‚æ•°
   â–¡ è°ƒç”¨ weight_loader åŠ è½½æƒé‡

âœ… æµ‹è¯•
   â–¡ èƒ½å¤ŸæˆåŠŸåŠ è½½HuggingFaceæƒé‡
   â–¡ æ¨ç†è¾“å‡ºå½¢çŠ¶æ­£ç¡®
   â–¡ æ”¯æŒtensorå¹¶è¡Œ (å¤šGPU)
   â–¡ æ”¯æŒé‡åŒ– (å¦‚æœé…ç½®äº†)
   â–¡ æ€§èƒ½ç¬¦åˆé¢„æœŸ
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6. è½¬æ¢å®æˆ˜ï¼šå¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## é—®é¢˜1: å¦‚ä½•å¤„ç†å¤šè¾“å‡ºå¤´ï¼Ÿ

### HuggingFaceå®ç°
```python
class ModelWithMultiHeads(PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.model = BaseModel(config)
        self.head1 = nn.Linear(config.hidden_size, config.num_labels_1)
        self.head2 = nn.Linear(config.hidden_size, config.num_labels_2)
    
    def forward(self, input_ids, ...):
        hidden = self.model(input_ids, ...)
        output1 = self.head1(hidden)
        output2 = self.head2(hidden)
        return (output1, output2)
```

### vLLMå®ç°
```python
class ModelWithMultiHeads(nn.Module):
    def __init__(self, config, cache_config, quant_config, lora_config):
        super().__init__()
        self.config = config
        self.model = BaseModel(config, cache_config, quant_config)
        
        # ä¸¤ä¸ªå¤´éƒ½ä½¿ç”¨å¹¶è¡Œå±‚
        self.head1 = RowParallelLinear(
            config.hidden_size,
            config.num_labels_1,
            quant_config=quant_config,
        )
        self.head2 = RowParallelLinear(
            config.hidden_size,
            config.num_labels_2,
            quant_config=quant_config,
        )
        
        # å¯ä»¥ä½¿ç”¨ä¸åŒçš„pooler
        self._pooler1 = Pooler(pooling_type=PoolingType.LAST, normalize=False)
        self._pooler2 = Pooler(pooling_type=PoolingType.MEAN, normalize=True)
    
    def forward(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors=None):
        hidden_states = self.model(input_ids, positions, kv_caches, attn_metadata, intermediate_tensors)
        
        # è§£åŒ…ä¸¤ä¸ªå¤´çš„è¾“å‡º
        output1, _ = self.head1(hidden_states)
        output2, _ = self.head2(hidden_states)
        
        # å¯ä»¥è¿”å›æ‹¼æ¥çš„å¼ é‡æˆ–å­—å…¸
        return torch.cat([output1, output2], dim=-1)
    
    def pooler(self, hidden_states, pooling_metadata):
        # å¦‚æœéœ€è¦ä¸åŒçš„æ± åŒ–ç­–ç•¥ï¼Œå¯ä»¥åˆ†åˆ«å¤„ç†
        # è¿™é‡Œç®€åŒ–ä¸ºä½¿ç”¨ç¬¬ä¸€ä¸ªpooler
        return self._pooler1(hidden_states, pooling_metadata)
```

## é—®é¢˜2: å¦‚ä½•å¤„ç†Dropoutå±‚ï¼Ÿ

**é‡è¦**: vLLMæ˜¯çº¯æ¨ç†æ¡†æ¶ï¼Œè®­ç»ƒæ—¶çš„Dropoutåœ¨æ¨ç†æ—¶åº”è¯¥è¢«å¿½ç•¥

### HuggingFaceå®ç°
```python
self.classifier = nn.Sequential(
    nn.Linear(hidden, intermediate),
    nn.Dropout(0.1),  # è®­ç»ƒæ—¶ä½¿ç”¨
    nn.Linear(intermediate, output),
)
```

### vLLMå®ç° - æ–¹æ¡ˆA: ç›´æ¥ç§»é™¤
```python
self.classifier = nn.Sequential(
    ColumnParallelLinear(hidden, intermediate, quant_config=quant_config),
    CustomActivation(),
    RowParallelLinear(intermediate, output, quant_config=quant_config),
)
# Dropoutåœ¨æ¨ç†æ—¶ç­‰ä»·äºæ’ç­‰æ˜ å°„ï¼Œç›´æ¥ç§»é™¤
```

### vLLMå®ç° - æ–¹æ¡ˆB: ä¿ç•™ä½†è®¾ç½®evalæ¨¡å¼
```python
self.classifier = nn.Sequential(
    ColumnParallelLinear(hidden, intermediate, quant_config=quant_config),
    nn.Dropout(0.1),  # ä¿ç•™ï¼Œä½†æ¨¡å‹ä¼šè‡ªåŠ¨åœ¨evalæ¨¡å¼ä¸‹ç¦ç”¨
    RowParallelLinear(intermediate, output, quant_config=quant_config),
)
# vLLMä¼šè‡ªåŠ¨å°†æ¨¡å‹è®¾ç½®ä¸ºevalæ¨¡å¼ï¼ŒDropoutä¼šè¢«ç¦ç”¨
```

## é—®é¢˜3: å¦‚ä½•å¤„ç†LayerNorm/RMSNormï¼Ÿ

æ ‡å‡†çš„å½’ä¸€åŒ–å±‚ä¸éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œç›´æ¥ä½¿ç”¨å³å¯ï¼š

```python
# HuggingFaceå’ŒvLLMé€šç”¨
self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
self.rms_norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

# åœ¨forwardä¸­æ­£å¸¸ä½¿ç”¨
normalized = self.layer_norm(hidden_states)
```

## é—®é¢˜4: å¦‚ä½•å¤„ç†æ¡ä»¶åˆ†æ”¯ï¼ˆif-elseï¼‰ï¼Ÿ

### HuggingFaceå®ç°
```python
def forward(self, input_ids, task_type=None, ...):
    hidden = self.model(input_ids, ...)
    
    if task_type == "classification":
        output = self.classifier(hidden)
    elif task_type == "regression":
        output = self.regressor(hidden)
    else:
        output = self.default_head(hidden)
    
    return output
```

### vLLMå®ç°
vLLMæ¨ç†æ—¶é€šå¸¸åªéœ€è¦ä¸€ä¸ªä»»åŠ¡è·¯å¾„ï¼Œæ‰€ä»¥ï¼š

**æ–¹æ¡ˆA: å›ºå®šä»»åŠ¡ç±»å‹**
```python
def __init__(self, config, cache_config, quant_config, lora_config):
    super().__init__()
    # åªåˆå§‹åŒ–éœ€è¦çš„å¤´
    task_type = getattr(config, "task_type", "classification")
    
    if task_type == "classification":
        self.head = RowParallelLinear(...)
    elif task_type == "regression":
        self.head = RowParallelLinear(...)
    
def forward(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors=None):
    hidden_states = self.model(...)
    output, _ = self.head(hidden_states)
    return output
```

**æ–¹æ¡ˆB: ä¿ç•™æ‰€æœ‰åˆ†æ”¯ï¼ˆå¦‚æœéœ€è¦ï¼‰**
```python
def __init__(self, config, cache_config, quant_config, lora_config):
    super().__init__()
    self.classifier = RowParallelLinear(...)
    self.regressor = RowParallelLinear(...)

def forward(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors=None):
    hidden_states = self.model(...)
    
    # é€šè¿‡é…ç½®æˆ–å¤–éƒ¨ä¿¡å·å†³å®šä½¿ç”¨å“ªä¸ªå¤´
    if self.config.task_type == "classification":
        output, _ = self.classifier(hidden_states)
    else:
        output, _ = self.regressor(hidden_states)
    
    return output
```

## é—®é¢˜5: å¦‚ä½•å¤„ç†è‡ªå®šä¹‰çš„åˆå§‹åŒ–ï¼Ÿ

### HuggingFaceå®ç°
```python
def __init__(self, config):
    super().__init__(config)
    self.model = BaseModel(config)
    self.classifier = nn.Linear(config.hidden_size, config.num_labels)
    
    # è‡ªå®šä¹‰åˆå§‹åŒ–
    self.post_init()

def _init_weights(self, module):
    if isinstance(module, nn.Linear):
        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
        if module.bias is not None:
            module.bias.data.zero_()
```

### vLLMå®ç°
vLLMä»checkpointåŠ è½½æƒé‡ï¼Œä¸éœ€è¦éšæœºåˆå§‹åŒ–ï¼š

```python
def __init__(self, config, cache_config, quant_config, lora_config):
    super().__init__()
    self.config = config
    self.model = BaseModel(config, cache_config, quant_config)
    self.classifier = RowParallelLinear(
        config.hidden_size,
        config.num_labels,
        quant_config=quant_config,
    )
    
    # ä¸éœ€è¦ post_init() æˆ– _init_weights()
    # æƒé‡ä¼šé€šè¿‡ load_weights() ä»checkpointåŠ è½½
```

## é—®é¢˜6: å¦‚ä½•å¤„ç†åµŒå¥—çš„Sequentialï¼Ÿ

### HuggingFaceå®ç°
```python
self.layers = nn.ModuleList([
    nn.Sequential(
        nn.Linear(hidden, intermediate),
        nn.ReLU(),
        nn.Linear(intermediate, hidden),
    )
    for _ in range(num_layers)
])
```

### vLLMå®ç°
```python
# è‡ªå®šä¹‰å—ç±»å¤„ç†å…ƒç»„
class TransformBlock(nn.Module):
    def __init__(self, hidden, intermediate, quant_config):
        super().__init__()
        self.up = ColumnParallelLinear(hidden, intermediate, quant_config=quant_config)
        self.act = CustomActivation()
        self.down = RowParallelLinear(intermediate, hidden, quant_config=quant_config)
    
    def forward(self, x):
        x, _ = self.up(x)
        x = self.act((x, None))  # åŒ…è£…ä¸ºå…ƒç»„ç»™æ¿€æ´»å‡½æ•°
        x, _ = self.down(x)
        return x

# ä½¿ç”¨è‡ªå®šä¹‰å—
self.layers = nn.ModuleList([
    TransformBlock(hidden, intermediate, quant_config)
    for _ in range(num_layers)
])
```

## é—®é¢˜7: å¦‚ä½•å¤„ç†è‡ªå®šä¹‰çš„å‰å‘é€»è¾‘ï¼ˆå¦‚æ®‹å·®è¿æ¥ï¼‰ï¼Ÿ

### HuggingFaceå®ç°
```python
def forward(self, input_ids, ...):
    hidden = self.model(input_ids, ...)
    
    # å¤æ‚çš„å‰å‘é€»è¾‘
    residual = hidden
    hidden = self.norm(hidden)
    hidden = self.ffn(hidden)
    hidden = residual + hidden
    
    output = self.head(hidden)
    return output
```

### vLLMå®ç°
å®Œå…¨ä¿ç•™ç›¸åŒçš„é€»è¾‘ï¼Œåªéœ€æ³¨æ„è§£åŒ…ï¼š

```python
def forward(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors=None):
    hidden_states = self.model(input_ids, positions, kv_caches, attn_metadata, intermediate_tensors)
    
    # ç›¸åŒçš„å‰å‘é€»è¾‘
    residual = hidden_states
    hidden_states = self.norm(hidden_states)
    hidden_states, _ = self.ffn(hidden_states)  # â† æ³¨æ„è§£åŒ…
    hidden_states = residual + hidden_states
    
    output, _ = self.head(hidden_states)  # â† æ³¨æ„è§£åŒ…
    return output
```

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 7. è°ƒè¯•æŠ€å·§
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## æŠ€å·§1: å¯¹æ¯”è¾“å‡ºå½¢çŠ¶

```python
# åœ¨HuggingFaceæ¨¡å‹ä¸­
print(f"HF hidden_states shape: {hidden_states.shape}")
print(f"HF logits shape: {logits.shape}")

# åœ¨vLLMæ¨¡å‹ä¸­
print(f"vLLM hidden_states shape: {hidden_states.shape}")
print(f"vLLM logits shape: {logits.shape}")

# åº”è¯¥å®Œå…¨ä¸€è‡´ï¼
```

## æŠ€å·§2: éªŒè¯æƒé‡åŠ è½½

```python
# åŠ è½½HuggingFaceæƒé‡åï¼Œæ£€æŸ¥å‚æ•°
for name, param in model.named_parameters():
    print(f"{name}: {param.shape}, requires_grad={param.requires_grad}")

# ç¡®ä¿:
# 1. æ‰€æœ‰å‚æ•°éƒ½è¢«åŠ è½½äº†
# 2. å †å å‚æ•°çš„å½¢çŠ¶æ­£ç¡® (å¦‚qkv_projåº”è¯¥æ˜¯3å€çš„hidden_size)
# 3. æ²¡æœ‰æœªåˆå§‹åŒ–çš„å‚æ•°
```

## æŠ€å·§3: å¯¹æ¯”æ•°å€¼è¾“å‡º

```python
import torch

# å‡†å¤‡ç›¸åŒçš„è¾“å…¥
input_ids = torch.randint(0, 1000, (2, 10))  # [batch=2, seq_len=10]

# HuggingFaceæ¨ç†
hf_model.eval()
with torch.no_grad():
    hf_output = hf_model(input_ids)
    hf_logits = hf_output.logits

# vLLMæ¨ç† (éœ€è¦å‡†å¤‡vLLMæ ¼å¼çš„è¾“å…¥)
vllm_model.eval()
with torch.no_grad():
    positions = torch.arange(10).unsqueeze(0).expand(2, -1)
    # ... å‡†å¤‡kv_cacheså’Œattn_metadata
    vllm_output = vllm_model(input_ids, positions, kv_caches, attn_metadata)

# å¯¹æ¯”ç»“æœ
print(f"Output difference: {torch.abs(hf_logits - vllm_output).max().item()}")
# åº”è¯¥éå¸¸å° (< 1e-4)
```

## æŠ€å·§4: é€å±‚éªŒè¯

```python
# åœ¨forwardä¸­æ·»åŠ è°ƒè¯•è¾“å‡º
def forward(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors=None):
    print(f"[DEBUG] input_ids shape: {input_ids.shape}")
    
    hidden_states = self.model(input_ids, positions, kv_caches, attn_metadata, intermediate_tensors)
    print(f"[DEBUG] hidden_states shape: {hidden_states.shape}")
    print(f"[DEBUG] hidden_states range: [{hidden_states.min():.4f}, {hidden_states.max():.4f}]")
    
    logits, bias = self.score(hidden_states)
    print(f"[DEBUG] logits shape: {logits.shape}")
    print(f"[DEBUG] bias shape: {bias.shape if bias is not None else None}")
    
    return logits
```

## æŠ€å·§5: æ£€æŸ¥å¹¶è¡Œå±‚çš„è¾“å‡º

```python
# æµ‹è¯•å¹¶è¡Œå±‚æ˜¯å¦æ­£ç¡®å¤„ç†å…ƒç»„
class TestModule(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = ColumnParallelLinear(768, 768)
        self.act = CustomActivation()
    
    def forward(self, x):
        print(f"Input shape: {x.shape}")
        
        out = self.linear(x)
        print(f"ColumnParallel output type: {type(out)}")
        print(f"ColumnParallel output[0] shape: {out[0].shape if isinstance(out, tuple) else 'not tuple'}")
        
        activated = self.act(out)
        print(f"After activation shape: {activated.shape}")
        
        return activated

# è¿è¡Œæµ‹è¯•
test = TestModule()
x = torch.randn(2, 10, 768)
output = test(x)
```

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 8. å®Œæ•´è½¬æ¢å®ä¾‹ï¼šä»é›¶å¼€å§‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå®Œæ•´çš„ä¾‹å­ï¼Œæ¼”ç¤ºå¦‚ä½•å°†ä¸€ä¸ªç®€å•çš„HuggingFaceåˆ†ç±»æ¨¡å‹è½¬æ¢ä¸ºvLLMå®ç°ã€‚

## åŸå§‹HuggingFaceå®ç°

```python
# ========== original_model.py (HuggingFace) ==========
from transformers import PreTrainedModel, LlamaModel
import torch
import torch.nn as nn

class SentimentClassifier(PreTrainedModel):
    """æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹ - HuggingFaceå®ç°"""
    
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        
        # éª¨å¹²ç½‘ç»œ
        self.model = LlamaModel(config)
        
        # åˆ†ç±»å¤´: 2å±‚MLP
        self.dropout = nn.Dropout(config.classifier_dropout)
        self.classifier = nn.Sequential(
            nn.Linear(config.hidden_size, config.hidden_size // 2),
            nn.Tanh(),
            nn.Dropout(config.classifier_dropout),
            nn.Linear(config.hidden_size // 2, self.num_labels)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self.post_init()
    
    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        position_ids=None,
        past_key_values=None,
        inputs_embeds=None,
        labels=None,
        use_cache=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ):
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        
        # 1. é€šè¿‡éª¨å¹²ç½‘ç»œæå–ç‰¹å¾
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        
        # 2. è·å–åºåˆ—è¡¨ç¤º
        hidden_states = outputs[0]  # [batch, seq_len, hidden_size]
        
        # 3. æå–æœ€åä¸€ä¸ªtoken (ç”¨äºåˆ†ç±»)
        batch_size = input_ids.shape[0] if input_ids is not None else inputs_embeds.shape[0]
        
        if self.config.pad_token_id is None and batch_size != 1:
            raise ValueError("Cannot handle batch sizes > 1 if no padding token is defined.")
        
        if self.config.pad_token_id is None:
            sequence_lengths = -1
        else:
            if input_ids is not None:
                sequence_lengths = (torch.eq(input_ids, self.config.pad_token_id).long().argmax(-1) - 1)
                sequence_lengths = sequence_lengths % input_ids.shape[-1]
                sequence_lengths = sequence_lengths.to(hidden_states.device)
            else:
                sequence_lengths = -1
        
        # 4. æ± åŒ–
        pooled_hidden_states = hidden_states[torch.arange(batch_size, device=hidden_states.device), sequence_lengths]
        
        # 5. Dropout
        pooled_hidden_states = self.dropout(pooled_hidden_states)
        
        # 6. åˆ†ç±»
        logits = self.classifier(pooled_hidden_states)
        
        # 7. è®¡ç®—æŸå¤±
        loss = None
        if labels is not None:
            if self.config.problem_type is None:
                if self.num_labels == 1:
                    self.config.problem_type = "regression"
                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
                    self.config.problem_type = "single_label_classification"
                else:
                    self.config.problem_type = "multi_label_classification"
            
            if self.config.problem_type == "regression":
                loss_fct = nn.MSELoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))
            elif self.config.problem_type == "single_label_classification":
                loss_fct = nn.CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            elif self.config.problem_type == "multi_label_classification":
                loss_fct = nn.BCEWithLogitsLoss()
                loss = loss_fct(logits, labels)
        
        # 8. è¿”å›ç»“æœ
        if not return_dict:
            output = (logits,) + outputs[1:]
            return ((loss,) + output) if loss is not None else output
        
        from transformers.modeling_outputs import SequenceClassifierOutputWithPast
        return SequenceClassifierOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )
```

## è½¬æ¢åçš„vLLMå®ç°

```python
# ========== vllm_model.py (vLLM) ==========
from typing import Iterable, List, Optional, Tuple

import torch
from torch import nn

# vLLM imports
from vllm.attention import AttentionMetadata
from vllm.config import CacheConfig, LoRAConfig
from vllm.model_executor.layers.linear import ColumnParallelLinear, RowParallelLinear
from vllm.model_executor.layers.pooler import Pooler, PoolingType
from vllm.model_executor.layers.quantization.base_config import QuantizationConfig
from vllm.model_executor.model_loader.weight_utils import (
    default_weight_loader,
    maybe_remap_kv_scale_name
)
from vllm.model_executor.models.llama import LlamaModel
from vllm.model_executor.pooling_metadata import PoolingMetadata
from vllm.sequence import IntermediateTensors, PoolerOutput

from .utils import is_pp_missing_parameter


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ­¥éª¤1: è‡ªå®šä¹‰Tanhæ¿€æ´» (å¤„ç†å¹¶è¡Œå±‚çš„å…ƒç»„è¾“å‡º)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class TanhActivation(nn.Module):
    """è‡ªå®šä¹‰Tanhæ¿€æ´»ï¼Œå¤„ç†å¹¶è¡Œå±‚çš„å…ƒç»„è¾“å‡º"""
    
    def __init__(self):
        super().__init__()
        self.activation = nn.Tanh()
    
    def forward(self, input):
        # å¦‚æœè¾“å…¥æ˜¯å…ƒç»„ (æ¥è‡ªColumnParallelLinear)
        if isinstance(input, tuple):
            input, _ = input
        return self.activation(input)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ­¥éª¤2: vLLMä¼˜åŒ–çš„æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class SentimentClassifier(nn.Module):
    """
    æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹ - vLLMä¼˜åŒ–å®ç°
    
    åŸå§‹å®ç°: HuggingFaceç‰ˆæœ¬çš„SentimentClassifier
    ä¼˜åŒ–: æ”¯æŒtensorå¹¶è¡Œã€é‡åŒ–ã€é«˜ååé‡æ‰¹é‡æ¨ç†
    """
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ç±»å±æ€§é…ç½®
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # Llamaæ¨¡å‹çš„å‚æ•°å †å æ˜ å°„
    packed_modules_mapping = {
        "qkv_proj": ["q_proj", "k_proj", "v_proj"],
        "gate_up_proj": ["gate_proj", "up_proj"],
    }
    
    # LoRAæ”¯æŒçš„æ¨¡å—
    supported_lora_modules = [
        "qkv_proj",
        "o_proj",
        "gate_up_proj",
        "down_proj",
        # åˆ†ç±»å¤´ä¹Ÿå¯ä»¥æ”¯æŒLoRA
        "classifier.0",  # ç¬¬ä¸€ä¸ªçº¿æ€§å±‚
        "classifier.2",  # ç¬¬äºŒä¸ªçº¿æ€§å±‚
    ]
    
    embedding_modules = {}
    embedding_padding_modules = []
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # åˆå§‹åŒ–
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def __init__(
        self,
        config,
        cache_config: Optional[CacheConfig] = None,
        quant_config: Optional[QuantizationConfig] = None,
        lora_config: Optional[LoRAConfig] = None,
    ) -> None:
        super().__init__()
        
        # ä¿å­˜é…ç½®
        self.config = config
        self.lora_config = lora_config
        self.quant_config = quant_config
        self.num_labels = config.num_labels
        
        # åˆå§‹åŒ–éª¨å¹²ç½‘ç»œ (Llama)
        self.model = LlamaModel(config, cache_config, quant_config)
        
        # åˆå§‹åŒ–åˆ†ç±»å¤´
        # æ³¨æ„:
        # 1. ä¸ä½¿ç”¨Dropout (æ¨ç†æ—¶ä¸éœ€è¦)
        # 2. ä½¿ç”¨å¹¶è¡Œçº¿æ€§å±‚
        # 3. ä½¿ç”¨è‡ªå®šä¹‰æ¿€æ´»å‡½æ•°
        self.classifier = nn.Sequential(
            # ç¬¬ä¸€å±‚: é™ç»´ (hidden_size â†’ hidden_size // 2)
            ColumnParallelLinear(
                config.hidden_size,
                config.hidden_size // 2,
                bias=True,
                quant_config=quant_config,
            ),
            # Tanhæ¿€æ´»
            TanhActivation(),
            # ç¬¬äºŒå±‚: åˆ†ç±» (hidden_size // 2 â†’ num_labels)
            RowParallelLinear(
                config.hidden_size // 2,
                self.num_labels,
                bias=True,
                quant_config=quant_config,
            ),
        )
        
        # åˆå§‹åŒ–Pooler (å–æœ€åä¸€ä¸ªtoken)
        self._pooler = Pooler(
            pooling_type=PoolingType.LAST,
            normalize=False,
        )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # å‰å‘ä¼ æ’­
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
        kv_caches: List[torch.Tensor],
        attn_metadata: AttentionMetadata,
        intermediate_tensors: Optional[IntermediateTensors] = None,
    ) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            input_ids: [batch_size, seq_len] è¾“å…¥token IDs
            positions: [batch_size, seq_len] ä½ç½®ç´¢å¼•
            kv_caches: KVç¼“å­˜åˆ—è¡¨
            attn_metadata: æ³¨æ„åŠ›å…ƒæ•°æ®
            intermediate_tensors: æµæ°´çº¿å¹¶è¡Œçš„ä¸­é—´å¼ é‡
        
        Returns:
            logits: [batch_size, seq_len, num_labels] åˆ†ç±»logits
        """
        # 1. é€šè¿‡éª¨å¹²ç½‘ç»œæå–ç‰¹å¾
        hidden_states = self.model(
            input_ids,
            positions,
            kv_caches,
            attn_metadata,
            intermediate_tensors
        )
        # hidden_states: [batch_size, seq_len, hidden_size]
        
        # 2. é€šè¿‡åˆ†ç±»å¤´è®¡ç®—logits
        # æ³¨æ„: æœ€åä¸€å±‚æ˜¯RowParallelLinearï¼Œè¿”å›å…ƒç»„
        logits, _ = self.classifier(hidden_states)
        # logits: [batch_size, seq_len, num_labels]
        
        return logits
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Pooler (æå–æœ€åä¸€ä¸ªtokençš„logits)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def pooler(
        self,
        hidden_states: torch.Tensor,
        pooling_metadata: PoolingMetadata,
    ) -> Optional[PoolerOutput]:
        """
        æ± åŒ–: ä»åºåˆ—è¾“å‡ºä¸­æå–æœ€åä¸€ä¸ªæœ‰æ•ˆtokençš„logits
        
        Args:
            hidden_states: [total_tokens, num_labels] æ‰€æœ‰tokençš„logits
            pooling_metadata: æ± åŒ–å…ƒæ•°æ® (åŒ…å«åºåˆ—è¾¹ç•Œä¿¡æ¯)
        
        Returns:
            PoolerOutput: åŒ…å«æ¯ä¸ªåºåˆ—æœ€åä¸€ä¸ªtokençš„logits
        """
        return self._pooler(hidden_states, pooling_metadata)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # æƒé‡åŠ è½½
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
        """
        ä»HuggingFace checkpointåŠ è½½æƒé‡
        
        Args:
            weights: è¿­ä»£å™¨ï¼Œäº§ç”Ÿ (å‚æ•°å, æƒé‡å¼ é‡) å¯¹
        """
        # å®šä¹‰å‚æ•°å †å æ˜ å°„
        stacked_params_mapping = [
            # Llamaçš„QKVå †å 
            ("qkv_proj", "q_proj", "q"),
            ("qkv_proj", "k_proj", "k"),
            ("qkv_proj", "v_proj", "v"),
            # Llamaçš„gate-upå †å 
            ("gate_up_proj", "gate_proj", 0),
            ("gate_up_proj", "up_proj", 1),
        ]
        
        # è·å–å‚æ•°å­—å…¸
        params_dict = dict(self.named_parameters(remove_duplicate=False))
        
        # éå†æƒé‡
        for name, loaded_weight in weights:
            # è·³è¿‡ä¸éœ€è¦çš„æƒé‡
            if name == "lm_head.weight":
                continue
            if "rotary_emb.inv_freq" in name:
                continue
            
            # å¤„ç†å †å å‚æ•°
            for (param_name, weight_name, shard_id) in stacked_params_mapping:
                if weight_name not in name:
                    continue
                
                name = name.replace(weight_name, param_name)
                
                if name.endswith(".bias") and name not in params_dict:
                    continue
                
                if is_pp_missing_parameter(name, self):
                    continue
                
                param = params_dict[name]
                weight_loader = param.weight_loader
                weight_loader(param, loaded_weight, shard_id)
                break
            else:
                # å¤„ç†å¸¸è§„å‚æ•°
                if name.endswith(".bias") and name not in params_dict:
                    continue
                
                name = maybe_remap_kv_scale_name(name, params_dict)
                if name is None:
                    continue
                
                if is_pp_missing_parameter(name, self):
                    continue
                
                param = params_dict[name]
                weight_loader = getattr(
                    param,
                    "weight_loader",
                    default_weight_loader
                )
                weight_loader(param, loaded_weight)
```

## è½¬æ¢å¯¹ç…§è¡¨

| ç»„ä»¶ | HuggingFace | vLLM | å˜åŒ–è¯´æ˜ |
|------|-------------|------|---------|
| **åŸºç±»** | `PreTrainedModel` | `nn.Module` | ç§»é™¤HFç‰¹å®šåŠŸèƒ½ |
| **__init__å‚æ•°** | `config` | `config, cache_config, quant_config, lora_config` | æ–°å¢æ¨ç†é…ç½® |
| **éª¨å¹²ç½‘ç»œ** | `LlamaModel(config)` | `LlamaModel(config, cache_config, quant_config)` | ä¼ å…¥æ¨ç†é…ç½® |
| **Dropout** | `nn.Dropout(0.1)` | ç§»é™¤ | æ¨ç†æ—¶ä¸éœ€è¦ |
| **ç¬¬1å±‚Linear** | `nn.Linear(hidden, hidden//2)` | `ColumnParallelLinear(...)` | æ”¯æŒå¹¶è¡Œ |
| **Tanh** | `nn.Tanh()` | `TanhActivation()` | å¤„ç†å…ƒç»„è¾“å…¥ |
| **ç¬¬2å±‚Linear** | `nn.Linear(hidden//2, num_labels)` | `RowParallelLinear(...)` | æ”¯æŒå¹¶è¡Œ |
| **forwardå‚æ•°** | 11ä¸ªå‚æ•° (labels, return_dictç­‰) | 5ä¸ªå‚æ•° (vLLMä¸“ç”¨) | ç®€åŒ–ä¸ºæ¨ç†å‚æ•° |
| **æ± åŒ–é€»è¾‘** | åœ¨forwardå†…éƒ¨ | ç‹¬ç«‹pooleræ–¹æ³• | é€»è¾‘è§£è€¦ |
| **æŸå¤±è®¡ç®—** | åœ¨forwardä¸­è®¡ç®— | ç§»é™¤ | çº¯æ¨ç† |
| **è¿”å›å€¼** | `SequenceClassifierOutputWithPast` | `torch.Tensor` | ç®€åŒ–è¾“å‡º |
| **æƒé‡åˆå§‹åŒ–** | `post_init()` | æ—  | ä»checkpointåŠ è½½ |
| **æ–°å¢æ–¹æ³•** | æ—  | `load_weights()` | è‡ªå®šä¹‰æƒé‡åŠ è½½ |

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 9. æ³¨å†Œæ¨¡å‹åˆ°vLLM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

è½¬æ¢å®Œæˆåï¼Œéœ€è¦å°†æ¨¡å‹æ³¨å†Œåˆ°vLLMæ¡†æ¶ï¼š

## æ­¥éª¤1: åˆ›å»ºæ¨¡å‹æ–‡ä»¶

```bash
# vLLMæ¨¡å‹æ–‡ä»¶ç»“æ„
vllm/model_executor/models/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ llama.py
â”œâ”€â”€ qwen2.py
â””â”€â”€ sentiment_classifier.py  # â† ä½ çš„æ–°æ¨¡å‹
```

## æ­¥éª¤2: åœ¨__init__.pyä¸­æ³¨å†Œ

```python
# vllm/model_executor/models/__init__.py

# ... å…¶ä»–å¯¼å…¥ ...

# æ³¨å†Œä½ çš„æ¨¡å‹
_MODEL_REGISTRY = {
    # ... ç°æœ‰æ¨¡å‹ ...
    "LlamaForCausalLM": ("llama", "LlamaForCausalLM"),
    "Qwen2ForRewardModel": ("qwen2_rm", "Qwen2ForRewardModel"),
    
    # æ–°å¢ä½ çš„æ¨¡å‹
    "SentimentClassifier": ("sentiment_classifier", "SentimentClassifier"),
}
```

## æ­¥éª¤3: é…ç½®æ¨¡å‹æ¶æ„

```python
# config.json ä¸­æŒ‡å®šæ¶æ„
{
  "architectures": [
    "SentimentClassifier"  # â† ä¸æ³¨å†Œåç§°åŒ¹é…
  ],
  "model_type": "llama",  # åŸºäºçš„éª¨å¹²ç½‘ç»œç±»å‹
  ...
}
```

## æ­¥éª¤4: ä½¿ç”¨æ¨¡å‹

```python
from vllm import LLM

# åˆå§‹åŒ–æ¨¡å‹
llm = LLM(
    model="path/to/your/model",
    task="classify",  # æˆ– "embed"
    tensor_parallel_size=2,  # ä½¿ç”¨2ä¸ªGPU
    quantization="fp8",  # å¯é€‰: ä½¿ç”¨FP8é‡åŒ–
)

# æ¨ç†
prompts = [
    "This movie is amazing!",
    "I hate this product.",
]

outputs = llm.encode(prompts)

for output in outputs:
    print(f"Text: {output.prompt}")
    print(f"Logits: {output.outputs[0]}")  # [num_labels]çš„å‘é‡
```

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 10. æ€§èƒ½ä¼˜åŒ–å»ºè®®
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## ä¼˜åŒ–1: ä½¿ç”¨FP8é‡åŒ–

```python
# å¯ç”¨FP8é‡åŒ–å¯ä»¥æ˜¾è‘—æå‡ååé‡
llm = LLM(
    model="your-model",
    quantization="fp8",  # æˆ– "int8", "gptq"
    tensor_parallel_size=4,
)

# æ€§èƒ½æå‡: 2-3xååé‡, 50%å†…å­˜å ç”¨
```

## ä¼˜åŒ–2: è°ƒæ•´æ‰¹æ¬¡å¤§å°

```python
# å¢å¤§max_num_seqsä»¥æé«˜ååé‡
llm = LLM(
    model="your-model",
    max_num_seqs=256,  # é»˜è®¤æ˜¯256ï¼Œå¯ä»¥å¢åŠ 
    max_model_len=512,  # æ ¹æ®ä½ çš„åºåˆ—é•¿åº¦è°ƒæ•´
)
```

## ä¼˜åŒ–3: å¯ç”¨è¿ç»­æ‰¹å¤„ç†

vLLMè‡ªåŠ¨å¯ç”¨continuous batchingï¼Œæ— éœ€é…ç½®ã€‚è¿™å…è®¸ï¼š
- åŠ¨æ€æ·»åŠ æ–°è¯·æ±‚åˆ°æ‰¹æ¬¡
- ä¸åŒé•¿åº¦çš„åºåˆ—é«˜æ•ˆå¤„ç†
- æœ€å¤§åŒ–GPUåˆ©ç”¨ç‡

## ä¼˜åŒ–4: ä½¿ç”¨PagedAttention

vLLMè‡ªåŠ¨ä½¿ç”¨PagedAttentionç®¡ç†KVç¼“å­˜ï¼š
- å‡å°‘å†…å­˜ç¢ç‰‡
- æ”¯æŒæ›´å¤§çš„æ‰¹æ¬¡
- æ¥è¿‘é›¶å†…å­˜æµªè´¹

## ä¼˜åŒ–5: Tensorå¹¶è¡Œæœ€ä½³å®è·µ

```python
# GPUæ•°é‡é€‰æ‹©å»ºè®®:
# - å°æ¨¡å‹ (< 13B): tensor_parallel_size=1
# - ä¸­ç­‰æ¨¡å‹ (13B-40B): tensor_parallel_size=2 æˆ– 4
# - å¤§æ¨¡å‹ (> 40B): tensor_parallel_size=4 æˆ– 8

# ç¡®ä¿æ¨¡å‹èƒ½å‡åŒ€åˆ‡åˆ†
assert config.hidden_size % tensor_parallel_size == 0
assert config.num_attention_heads % tensor_parallel_size == 0
```

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 11. å¸¸è§é”™è¯¯ä¸è§£å†³æ–¹æ¡ˆ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## é”™è¯¯1: å½¢çŠ¶ä¸åŒ¹é…

```
RuntimeError: The size of tensor a (X) must match the size of tensor b (Y)
```

**åŸå› **: å¹¶è¡Œå±‚è¿”å›å…ƒç»„ï¼Œä½†ç›´æ¥å½“ä½œå¼ é‡ä½¿ç”¨

**è§£å†³**:
```python
# âŒ é”™è¯¯
output = self.layer(input)
result = output + residual  # outputæ˜¯å…ƒç»„ï¼

# âœ… æ­£ç¡®
output, _ = self.layer(input)
result = output + residual
```

## é”™è¯¯2: æƒé‡åŠ è½½å¤±è´¥

```
KeyError: 'qkv_proj.weight' not found in state dict
```

**åŸå› **: å‚æ•°åæ˜ å°„ä¸æ­£ç¡®

**è§£å†³**:
æ£€æŸ¥`stacked_params_mapping`æ˜¯å¦åŒ…å«æ‰€æœ‰éœ€è¦å †å çš„å‚æ•°ï¼š
```python
stacked_params_mapping = [
    ("qkv_proj", "q_proj", "q"),  # â† ç¡®ä¿è¿™äº›æ˜ å°„æ­£ç¡®
    ("qkv_proj", "k_proj", "k"),
    ("qkv_proj", "v_proj", "v"),
]
```

## é”™è¯¯3: PoolingTypeä¸åŒ¹é…

```
ValueError: Cannot pool sequences with different lengths
```

**åŸå› **: Poolingç±»å‹é€‰æ‹©ä¸å½“

**è§£å†³**:
```python
# åºåˆ—åˆ†ç±»: ä½¿ç”¨LAST
self._pooler = Pooler(pooling_type=PoolingType.LAST, normalize=False)

# Tokenåˆ†ç±»: ä½¿ç”¨ALL
self._pooler = Pooler(pooling_type=PoolingType.ALL, normalize=False)

# å¥å­åµŒå…¥: ä½¿ç”¨MEAN + å½’ä¸€åŒ–
self._pooler = Pooler(pooling_type=PoolingType.MEAN, normalize=True)
```

## é”™è¯¯4: CUDA OOM

```
RuntimeError: CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆ**:
1. å‡å°batch size: `max_num_seqs=128`
2. å‡å°åºåˆ—é•¿åº¦: `max_model_len=512`
3. å¯ç”¨é‡åŒ–: `quantization="fp8"`
4. å¢åŠ GPUæ•°é‡: `tensor_parallel_size=4`

## é”™è¯¯5: æ¿€æ´»å‡½æ•°é”™è¯¯

```
TypeError: forward() takes 2 positional arguments but 3 were given
```

**åŸå› **: æ ‡å‡†æ¿€æ´»å‡½æ•°ä¸èƒ½å¤„ç†å…ƒç»„è¾“å…¥

**è§£å†³**:
```python
# âŒ é”™è¯¯
self.act = nn.ReLU()

# âœ… æ­£ç¡®
class CustomReLU(nn.Module):
    def __init__(self):
        super().__init__()
        self.activation = nn.ReLU()
    
    def forward(self, input):
        if isinstance(input, tuple):
            input, _ = input
        return self.activation(input)

self.act = CustomReLU()
```

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 12. è½¬æ¢æ€»ç»“ä¸æ£€æŸ¥æ¸…å•
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## âœ… è½¬æ¢å‰æ£€æŸ¥æ¸…å•

- [ ] ç†è§£åŸå§‹HFæ¨¡å‹çš„æ¶æ„
- [ ] è¯†åˆ«æ‰€æœ‰çº¿æ€§å±‚çš„ä½ç½®
- [ ] è¯†åˆ«æ‰€æœ‰æ¿€æ´»å‡½æ•°
- [ ] è¯†åˆ«æ± åŒ–é€»è¾‘
- [ ] äº†è§£æ¨¡å‹çš„è¾“å…¥è¾“å‡ºæ ¼å¼
- [ ] æ£€æŸ¥æ˜¯å¦æœ‰æ¡ä»¶åˆ†æ”¯
- [ ] æ£€æŸ¥æ˜¯å¦æœ‰è‡ªå®šä¹‰åˆå§‹åŒ–

## âœ… è½¬æ¢è¿‡ç¨‹æ£€æŸ¥æ¸…å•

- [ ] å°†åŸºç±»æ”¹ä¸º`nn.Module`
- [ ] æ·»åŠ `packed_modules_mapping`
- [ ] æ·»åŠ `supported_lora_modules`
- [ ] ä¿®æ”¹`__init__`å‚æ•°ï¼ˆå¢åŠ cache_configç­‰ï¼‰
- [ ] å°†`nn.Linear`æ›¿æ¢ä¸º`ColumnParallelLinear`/`RowParallelLinear`
- [ ] åŒ…è£…æ¿€æ´»å‡½æ•°ä»¥å¤„ç†å…ƒç»„
- [ ] ç§»é™¤Dropoutå±‚ï¼ˆæˆ–ä¿ç•™ä½†ä¸å½±å“æ¨ç†ï¼‰
- [ ] ä¿®æ”¹`forward`ç­¾åï¼ˆä½¿ç”¨vLLMå‚æ•°ï¼‰
- [ ] è§£åŒ…æ‰€æœ‰å¹¶è¡Œå±‚çš„è¾“å‡º
- [ ] å°†æ± åŒ–é€»è¾‘ç§»åˆ°`pooler`æ–¹æ³•
- [ ] ç§»é™¤æŸå¤±è®¡ç®—å’Œæ ‡ç­¾å¤„ç†
- [ ] ç®€åŒ–è¿”å›å€¼ï¼ˆç›´æ¥è¿”å›å¼ é‡ï¼‰
- [ ] å®ç°`load_weights`æ–¹æ³•
- [ ] æ·»åŠ å‚æ•°å †å æ˜ å°„é€»è¾‘

## âœ… è½¬æ¢å